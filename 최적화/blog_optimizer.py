"""
ë¸”ë¡œê·¸ ì›ê³  ìë™ ìµœì í™” ì‹œìŠ¤í…œ
- ê¸ˆì¹™ì–´ ìë™ ì¹˜í™˜
- SEO ìµœì í™” (í‚¤ì›Œë“œ ë°˜ë³µ, í•´ì‹œíƒœê·¸ ë“±)
- AI ëŠë‚Œ ì œê±° (ë¬¸ì¥ íŒ¨í„´ ë‹¤ì–‘í™”)
"""

import re
import random
import os
from typing import Dict, List, Tuple
import pandas as pd
from forbidden_words_loader import ForbiddenWordsLoader


class BlogOptimizer:
    def __init__(self, forbidden_words_file='ê¸ˆì¹™ì–´ ë¦¬ìŠ¤íŠ¸.xlsx'):
        """ì´ˆê¸°í™”"""
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if not os.path.isabs(forbidden_words_file):
            base_dir = os.path.dirname(os.path.abspath(__file__))
            forbidden_words_file = os.path.join(base_dir, forbidden_words_file)

        # ìƒˆë¡œìš´ ê¸ˆì¹™ì–´ ë¡œë” ì‚¬ìš©
        self.forbidden_loader = ForbiddenWordsLoader(forbidden_words_file)

        # AI ëŠë‚Œ ë‚˜ëŠ” í‘œí˜„ë“¤ (ë‹¤ì–‘í™” í•„ìš”)
        self.ai_patterns = {
            'ì •ë§ ê³ ë¯¼ì´ ë§ìŠµë‹ˆë‹¤': [
                'ì •ë§ ê³ ë¯¼ë¼ìš”',
                'ì–´ë–»ê²Œ í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´ìš”',
                'ìƒê°ì´ ë§ì•„ì ¸ìš”'
            ],
            'ì ˆë¡œ ë‚˜ì˜¤': [
                'ìì—°ìŠ¤ëŸ½ê²Œ ë‚˜ì˜¤',
                'ì €ë„ ëª¨ë¥´ê²Œ ë‚˜ì˜¤',
                'ë¬´ì‹¬ì½” ë‚˜ì˜¤'
            ],
            'ê³ ìƒí•˜ê³  ìˆëŠ”': [
                'í˜ë“¤ì–´í•˜ëŠ”',
                'ì–´ë ¤ì›€ì„ ê²ªëŠ”',
                'ë¶ˆí¸í•¨ì„ ëŠë¼ëŠ”'
            ],
            'ì´ë ‡ê²Œ ê¸€ì„ ì˜¬ë ¤ë´…ë‹ˆë‹¤': [
                'ì—¬ì­¤ë³´ê³  ì‹¶ì–´ì„œìš”',
                'ê¶ê¸ˆí•´ì„œ ê¸€ ë‚¨ê²¨ìš”',
                'ì¡°ì–¸ êµ¬í•˜ëŸ¬ ì™”ì–´ìš”'
            ],
            'ì†”ì§íˆ': [
                'ì‚¬ì‹¤',
                'ì‹¤ì œë¡œ',
                'ìˆëŠ” ê·¸ëŒ€ë¡œ ë§í•˜ë©´'
            ],
            'ì •ë§': [
                'ì§„ì§œ',
                'í™•ì‹¤íˆ',
                'ë¶„ëª…íˆ'
            ],
            'ë„ˆë¬´': [
                'ì—„ì²­',
                'ë§ì´',
                'êµ‰ì¥íˆ'
            ]
        }

    def replace_forbidden_words(self, text: str) -> Tuple[str, List[str]]:
        """ê¸ˆì¹™ì–´ ì¹˜í™˜ (ìƒˆë¡œìš´ ë¡œë” ì‚¬ìš©)"""
        return self.forbidden_loader.replace_forbidden_words(text)

    def diversify_ai_patterns(self, text: str) -> Tuple[str, List[str]]:
        """AI ëŠë‚Œ ë‚˜ëŠ” íŒ¨í„´ ë‹¤ì–‘í™”"""
        diversified = []

        for pattern, alternatives in self.ai_patterns.items():
            if pattern in text:
                replacement = random.choice(alternatives)
                text = text.replace(pattern, replacement, 1)  # ì²« ë²ˆì§¸ë§Œ êµì²´
                diversified.append(f"{pattern} â†’ {replacement}")

        return text, diversified

    def optimize_keyword_density(self, text: str, keyword: str, target_count: int = 5) -> Tuple[str, int]:
        """í‚¤ì›Œë“œ ë°€ë„ ìµœì í™”"""
        if not keyword or pd.isna(keyword):
            return text, 0

        # í˜„ì¬ í‚¤ì›Œë“œ ì¶œí˜„ íšŸìˆ˜
        current_count = text.count(keyword)

        if current_count >= target_count:
            return text, current_count

        # í‚¤ì›Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ìœ„ì¹˜ ì°¾ê¸°
        # 1) "ì´ëŸ°", "ì´ê±°", "ê·¸ê±°", "ê·¸ëŸ°" ë“±ì„ í‚¤ì›Œë“œë¡œ êµì²´
        pronouns = ['ì´ëŸ°', 'ì´ê±°', 'ê·¸ê±°', 'ê·¸ëŸ°', 'ê·¸ê²Œ', 'ì´ê²Œ']
        added = 0

        for pronoun in pronouns:
            if added >= (target_count - current_count):
                break
            if pronoun in text:
                # ì²« ë²ˆì§¸ ë°œê²¬ëœ ëŒ€ëª…ì‚¬ë§Œ êµì²´
                text = text.replace(pronoun, keyword, 1)
                added += 1

        final_count = text.count(keyword)
        return text, final_count

    def add_natural_variations(self, text: str) -> str:
        """ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ë³€í˜• ì¶”ê°€"""
        # ë™ì¼í•œ ë¬¸ì¥ íŒ¨í„´ ë°©ì§€
        text = re.sub(r'(ì •ë§|ë„ˆë¬´|êµ‰ì¥íˆ)\s+(ì •ë§|ë„ˆë¬´|êµ‰ì¥íˆ)', r'\1', text)

        # "~í•˜ë”ë¼ê³ ìš”" ê³¼ë‹¤ ì‚¬ìš© ë°©ì§€
        count = text.count('í•˜ë”ë¼ê³ ìš”')
        if count > 2:
            alternatives = ['í•˜ë”êµ°ìš”', 'í–ˆì–´ìš”', 'í–ˆìŠµë‹ˆë‹¤', 'í–ˆì£ ']
            for i in range(count - 2):
                text = text.replace('í•˜ë”ë¼ê³ ìš”', random.choice(alternatives), 1)

        # "~ë„¤ìš”" ê³¼ë‹¤ ì‚¬ìš© ë°©ì§€
        count = text.count('ë„¤ìš”')
        if count > 3:
            alternatives = ['ì–´ìš”', 'ìŠµë‹ˆë‹¤', 'ì£ ']
            for i in range(count - 3):
                text = text.replace('ë„¤ìš”', random.choice(alternatives), 1)

        return text

    def generate_title(self, keyword: str, original_text: str) -> str:
        """SEO ìµœì í™” ì œëª© ìƒì„± (15-40ì ê¶Œì¥)"""
        if not keyword or pd.isna(keyword):
            return ''

        # ì œëª© í…œí”Œë¦¿ (ìƒí’ˆ íŒë§¤ìš©)
        templates = [
            f"{keyword} ì¶”ì²œ ì •ë³´ (í›„ê¸° ëª¨ìŒ)",
            f"{keyword} ì–´ë–¤ ê²Œ ì¢‹ì„ê¹Œìš”?",
            f"{keyword} ì •ë³´ ê³µìœ ",
            f"{keyword} ì‚¬ìš© ê²½í—˜ë‹´",
            f"{keyword} ì´ê±° ì–´ë–¤ê°€ìš”?",
            f"{keyword} ê´€ë ¨ ê¶ê¸ˆí•œ ì ",
            f"{keyword} ì •ë³´ ì°¾ì•„ë´¤ì–´ìš”",
        ]

        # ëœë¤ìœ¼ë¡œ í•˜ë‚˜ ì„ íƒ
        title = random.choice(templates)

        # 15-40ì ë²”ìœ„ í™•ì¸
        if len(title) < 15:
            title += " (ì†”ì§ í›„ê¸°)"
        elif len(title) > 40:
            title = title[:40]

        return title

    def generate_hashtags(self, keyword: str, brand: str) -> List[str]:
        """SEO ìµœì í™” í•´ì‹œíƒœê·¸ ìƒì„± (8-10ê°œ ê¶Œì¥)"""
        hashtags = []

        if not pd.isna(keyword):
            # ë©”ì¸ í‚¤ì›Œë“œ
            hashtags.append(keyword)

            # í‚¤ì›Œë“œ ì¡°ê° ë¶„ë¦¬
            keyword_parts = keyword.split()
            hashtags.extend(keyword_parts)

        if not pd.isna(brand):
            hashtags.append(brand)

        # ê´€ì ˆ/ê±´ê°• ê´€ë ¨ ì¼ë°˜ í•´ì‹œíƒœê·¸
        general_tags = [
            'ê±´ê°•ì •ë³´',
            'ê±´ê°•ê´€ë¦¬',
            'ì¼ìƒ',
            'í›„ê¸°',
            'ì •ë³´ê³µìœ ',
            'ì¶”ì²œ',
            'ê´€ì ˆê±´ê°•',
            'ê±´ê°•ì‹í’ˆ'
        ]

        # ì¤‘ë³µ ì œê±°í•˜ê³  8-10ê°œ ë§ì¶”ê¸°
        hashtags = list(dict.fromkeys(hashtags))  # ì¤‘ë³µ ì œê±°

        # ë¶€ì¡±í•˜ë©´ ì¼ë°˜ íƒœê·¸ ì¶”ê°€
        while len(hashtags) < 8:
            tag = random.choice([t for t in general_tags if t not in hashtags])
            hashtags.append(tag)

        # ë„ˆë¬´ ë§ìœ¼ë©´ ìë¥´ê¸°
        hashtags = hashtags[:10]

        return hashtags

    def optimize_text(self, text: str, keyword: str = '', brand: str = '', title: str = '') -> Dict:
        """í…ìŠ¤íŠ¸ ì „ì²´ ìµœì í™”"""
        if pd.isna(text) or not text:
            return {
                'optimized_text': '',
                'optimized_title': '',
                'changes': [],
                'keyword_count': 0,
                'hashtags': []
            }

        original_text = text
        changes = []

        # 1. ê¸ˆì¹™ì–´ ì¹˜í™˜
        text, forbidden_changes = self.replace_forbidden_words(text)
        changes.extend(forbidden_changes)

        # 2. AI íŒ¨í„´ ë‹¤ì–‘í™”
        text, ai_changes = self.diversify_ai_patterns(text)
        changes.extend(ai_changes)

        # 3. í‚¤ì›Œë“œ ë°€ë„ ìµœì í™”
        text, keyword_count = self.optimize_keyword_density(text, keyword)
        if keyword_count > 0:
            changes.append(f"í‚¤ì›Œë“œ '{keyword}' ì¶œí˜„: {keyword_count}íšŒ")

        # 4. ìì—°ìŠ¤ëŸ¬ìš´ ë³€í˜•
        text = self.add_natural_variations(text)

        # 5. í•´ì‹œíƒœê·¸ ìƒì„±
        hashtags = self.generate_hashtags(keyword, brand)

        # 6. ì œëª© ìƒì„± (ì—†ëŠ” ê²½ìš°)
        if pd.isna(title) or not title:
            title = self.generate_title(keyword, text)

        return {
            'optimized_text': text,
            'optimized_title': title,
            'original_length': len(original_text),
            'optimized_length': len(text),
            'changes': changes,
            'keyword_count': keyword_count,
            'hashtags': hashtags
        }

    def optimize_excel(self, input_file: str, output_file: str = None) -> Dict:
        """ì—‘ì…€ íŒŒì¼ ì „ì²´ ìµœì í™”"""
        if output_file is None:
            output_file = input_file.replace('.xlsx', '_ìµœì í™”.xlsx')

        # ì—‘ì…€ ì½ê¸°
        df = pd.read_excel(input_file)

        results = []

        # ê° í–‰ ìµœì í™”
        for idx, row in df.iterrows():
            keyword = row.get('í‚¤ì›Œë“œ', '')
            brand = row.get('ë¸Œëœë“œ', '')
            original_text = row.get('ì›ê³ ', '')
            title = row.get('ì œëª©', '')

            # ìµœì í™” ì‹¤í–‰
            result = self.optimize_text(original_text, keyword, brand, title)

            # ê²°ê³¼ ì €ì¥
            df.at[idx, 'ì›ê³ '] = result['optimized_text']

            # ì œëª© ì¶”ê°€/ì—…ë°ì´íŠ¸
            if result['optimized_title']:
                df.at[idx, 'ì œëª©'] = result['optimized_title']

            # í•´ì‹œíƒœê·¸ ì¶”ê°€ (ìƒˆ ì»¬ëŸ¼)
            df.at[idx, 'ì¶”ì²œ_í•´ì‹œíƒœê·¸'] = ' #'.join([''] + result['hashtags'])

            # ë³€ê²½ ì‚¬í•­ ê¸°ë¡
            df.at[idx, 'ìµœì í™”_ë³€ê²½ì‚¬í•­'] = '\n'.join(result['changes'])

            results.append({
                'row': idx + 1,
                'keyword': keyword,
                'keyword_count': result['keyword_count'],
                'changes_count': len(result['changes']),
                'hashtags_count': len(result['hashtags'])
            })

        # ì—‘ì…€ ì €ì¥
        df.to_excel(output_file, index=False)

        return {
            'input_file': input_file,
            'output_file': output_file,
            'total_rows': len(df),
            'results': results
        }


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    optimizer = BlogOptimizer()

    # ì—‘ì…€ ìµœì í™”
    result = optimizer.optimize_excel('ì‘ì—… ì˜ë¢°ìš© ë°ì´í„°.xlsx')

    print("\n" + "=" * 80)
    print("ğŸ‰ ë¸”ë¡œê·¸ ì›ê³  ìµœì í™” ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nğŸ“‚ ì…ë ¥ íŒŒì¼: {result['input_file']}")
    print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {result['output_file']}")
    print(f"ğŸ“Š ì²˜ë¦¬ëœ í–‰: {result['total_rows']}ê°œ")
    print("\n" + "=" * 80)
    print("ê° í–‰ë³„ ìµœì í™” ê²°ê³¼:")
    print("=" * 80)

    for r in result['results']:
        print(f"\n[{r['row']}í–‰] í‚¤ì›Œë“œ: {r['keyword']}")
        print(f"  âœ… í‚¤ì›Œë“œ ì¶œí˜„: {r['keyword_count']}íšŒ")
        print(f"  âœ… ë³€ê²½ ì‚¬í•­: {r['changes_count']}ê±´")
        print(f"  âœ… í•´ì‹œíƒœê·¸: {r['hashtags_count']}ê°œ")

    print("\n" + "=" * 80)
    print("âœ… ìµœì í™” ì™„ë£Œ!")
    print("ğŸ“ ì—‘ì…€ íŒŒì¼ì„ ì—´ì–´ì„œ ë‹¤ìŒ ì»¬ëŸ¼ì„ í™•ì¸í•˜ì„¸ìš”:")
    print("   - ì›ê³ : ìµœì í™”ëœ ì›ê³ ")
    print("   - ì œëª©: SEO ìµœì í™” ì œëª©")
    print("   - ì¶”ì²œ_í•´ì‹œíƒœê·¸: 8-10ê°œì˜ ì¶”ì²œ í•´ì‹œíƒœê·¸")
    print("   - ìµœì í™”_ë³€ê²½ì‚¬í•­: ê¸ˆì¹™ì–´ ì¹˜í™˜ ë“± ë³€ê²½ ë‚´ì—­")
    print("=" * 80 + "\n")


if __name__ == '__main__':
    main()
